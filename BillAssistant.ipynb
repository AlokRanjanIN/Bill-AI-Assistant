{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Packages Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwQ-Tzinz2pQ",
        "outputId": "65e67b0a-9dbb-4bb0-a3a2-c834cdedc13c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ALL REQUIRED PACKAGES INSTALLED!!!\n"
          ]
        }
      ],
      "source": [
        "!uv pip install -q addict \\\n",
        "    transformers==4.46.3 \\\n",
        "    tokenizers==0.20.3 \\\n",
        "    PyMuPDF \\\n",
        "    img2pdf \\\n",
        "    einops \\\n",
        "    easydict \\\n",
        "    Pillow \\\n",
        "    numpy \\\n",
        "    groq \\\n",
        "    streamlit \\\n",
        "    rank-bm25 \\\n",
        "    nltk \\\n",
        "    flash-attn==2.7.3 --no-build-isolation\n",
        "\n",
        "print(f\"ALL REQUIRED PACKAGES INSTALLED!!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the assistant in notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rmuYVUNx1-s4",
        "outputId": "8d290bf8-0528-4c51-e50f-7c69371dab26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Assistant Eye...\t‚úÖ ASSISTANT EYE LOADED SUCCESSFULLY!!!\n",
            "Loading Assistant Brain...\t‚úÖ ASSISTANT BRAIN LOADED SUCCESSFULLY!!!\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Compute capability: (7, 5)\n",
            "‚úì Flash Attention version: 2.7.3\n",
            "\n",
            "==================================================\n",
            "BILL ASSISTANT\n",
            "==================================================\n",
            "\n",
            "1. üì∏ Process bill image/PDF\n",
            "2. üíæ Export to JSON\n",
            "3. üí¨ Chat\n",
            "4. üö™ Exit\n",
            "\n",
            "Select option (1-4): 1\n",
            "Enter image/PDF path (or URL): /content/img.jpg\n",
            "üìù Extracting text...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====================\n",
            "BASE:  torch.Size([1, 256, 1280])\n",
            "NO PATCHES\n",
            "=====================\n",
            "Extractin complete.\n",
            "üß† Parsing with AI...\n",
            "\n",
            "\n",
            "PROMPT:\n",
            "\n",
            "                Extract structured data from this bill text as JSON:\n",
            "                East Repair Inc.  \n",
            "1912 Harvest Lane  \n",
            "New York, NY 12210  \n",
            "\n",
            "Bill To  \n",
            "John Smith  \n",
            "2 Court Square  \n",
            "New York, NY 12210  \n",
            "\n",
            "Ship To  \n",
            "John Smith  \n",
            "3787 Pineview Drive  \n",
            "Cambridge, MA 12210  \n",
            "\n",
            "Invoice #  \n",
            "US-001  \n",
            "\n",
            "Invoice Date  \n",
            "11/02/2019  \n",
            "\n",
            "P.O.#  \n",
            "2312/2019  \n",
            "\n",
            "Due Date  \n",
            "26/02/2019  \n",
            "\n",
            "| QTY | DESCRIPTION | UNIT PRICE | AMOUNT |\n",
            "|-----|-------------|------------|--------|\n",
            "| 1   | Front and rear brake cables | 100.00 | 100.00 |\n",
            "| 2   | New set of pedal arms | 15.00 | 30.00 |\n",
            "| 3   | Labor 3hrs | 5.00 | 15.00 |\n",
            "\n",
            "Subtotal  \n",
            "145.00  \n",
            "\n",
            "Sales Tax 6.25%  \n",
            "9.06  \n",
            "\n",
            "TOTAL  \n",
            "$154.06  \n",
            "\n",
            "Terms & Conditions  \n",
            "Payment is due within 15 days  \n",
            "\n",
            "Please make checks payable to: East Repair Inc.\n",
            "\n",
            "                Required fields:\n",
            "                - invoice_number, invoice_date, due_date, po_number\n",
            "                - bill_to (name, address, phone)\n",
            "                - ship_to (name, address, phone)\n",
            "                - items (qty, description, unit_price, amount)\n",
            "                - subtotal, tax, total\n",
            "                - company_name, company_address\n",
            "\n",
            "                Return ONLY valid JSON with these fields. Use empty strings for missing data.\n",
            "                \n",
            "\n",
            "\n",
            "=== Initial repaired ===\n",
            "{\n",
            "  \"invoice_number\": \"US-001\",\n",
            "  \"invoice_date\": \"11/02/2019\",\n",
            "  \"due_date\": \"26/02/2019\",\n",
            "  \"po_number\": \"2312/2019\",\n",
            "  \"bill_to\": {\n",
            "    \"name\": \"John Smith\",\n",
            "    \"address\": \"2 Court Square\",\n",
            "    \"city\": \"New York\",\n",
            "    \"state\": \"NY\",\n",
            "    \"zip\": \"12210\",\n",
            "    \"phone\": \"\"},\"ship_to\": {\n",
            "    \"name\": \"John Smith\",\n",
            "    \"address\": \"3787 Pineview Drive\",\n",
            "    \"city\": \"Cambridge\",\n",
            "    \"state\": \"MA\",\n",
            "    \"zip\": \"12210\",\n",
            "    \"phone\": \"\"},\"items\": [\n",
            "    {\n",
            "      \"qty\": 1,\n",
            "      \"description\": \"Front and rear brake cables\",\n",
            "      \"unit_price\": 100.00,\n",
            "      \"amount\": 100.00\n",
            "    },\n",
            "    {\n",
            "      \"qty\": 2,\n",
            "      \"description\": \"New set of pedal arms\",\n",
            "      \"unit_price\": 15.00,\n",
            "      \"amount\": 30.00\n",
            "    },\n",
            "    {\n",
            "      \"qty\": 3,\n",
            "=== Trying json.loads ===\n",
            "Recovered by trimming 148 chars.\n",
            "Parsed OK: {'invoice_number': 'US-001', 'invoice_date': '11/02/2019', 'due_date': '26/02/2019', 'po_number': '2312/2019', 'bill_to': {'name': 'John Smith', 'address': '2 Court Square', 'city': 'New York', 'state': 'NY', 'zip': '12210', 'phone': ''}, 'ship_to': {'name': 'John Smith', 'address': '3787 Pineview Drive', 'city': 'Cambridge', 'state': 'MA', 'zip': '12210', 'phone': ''}}\n",
            "üîç Creating text chunks...\n",
            "üî¢ 2 chunks created. Computing embeddings...\n",
            "üß† Computing semantic embeddings...\n",
            "üìö Building BM25 index for hybrid search...\n",
            "‚úÖ BM25 index built with 2 chunks!\n",
            "‚úÖ Bill processed successfully!\n",
            "\n",
            "üìä Bill Summary:\n",
            "Invoice: US-001\n",
            "Date: 11/02/2019\n",
            "Total: $N/A\n",
            "\n",
            "==================================================\n",
            "BILL ASSISTANT\n",
            "==================================================\n",
            "\n",
            "1. üì∏ Process bill image/PDF\n",
            "2. üíæ Export to JSON\n",
            "3. üí¨ Chat\n",
            "4. üö™ Exit\n",
            "\n",
            "Select option (1-4): 3\n",
            "\n",
            "üí¨ Chat mode activated (type 'exit' or 'quit' to return to main menu)\n",
            "ü§ñ Assistant: Hello! I'm your bill assistant. How can I help you today?\n",
            "You: hi baby\n",
            "ü§ñ Assistant: Not found\n",
            "You: hi\n",
            "ü§ñ Assistant: üëã Hello! I'm your bill assistant. I can help you extract data from bills, answer questions, and export information.\n",
            "You: what is the invoice date?\n",
            "ü§ñ Assistant: üìÖ Date: 11/02/2019\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4270569170.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0massistant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBillAssistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_colab_secrets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m     \u001b[0massistant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4270569170.py\u001b[0m in \u001b[0;36mrun_cli\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"3\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Call the new continuous chat loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m                 \u001b[0;31m# Show menu again after exiting chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4270569170.py\u001b[0m in \u001b[0;36mchat_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bye'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'goodbye'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "import tempfile  # For PDF processing\n",
        "\n",
        "# Basic environment / warning control\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\", message=\".*GetPrototype.*\")\n",
        "\n",
        "# Third-party libs\n",
        "import torch\n",
        "import gc\n",
        "try:\n",
        "    import flash_attn  # optional\n",
        "except Exception:\n",
        "    flash_attn = None\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "from transformers import AutoModel, AutoTokenizer, logging as hf_logging\n",
        "hf_logging.set_verbosity_error()\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Aux libs used in CLI\n",
        "import requests\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Optional Colab secrets and Groq client (only used if available)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    colab_secrets_available = True\n",
        "except Exception:\n",
        "    colab_secrets_available = False\n",
        "\n",
        "try:\n",
        "    from groq import Groq\n",
        "except Exception:\n",
        "    Groq = None\n",
        "\n",
        "# PDF processing dependencies\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "    PDF_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDF_SUPPORT = False\n",
        "    print(\"‚ö†Ô∏è PyMuPDF not installed. PDF processing disabled. Install with: pip install PyMuPDF\")\n",
        "\n",
        "# Hybrid Search Imports\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    import string\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    HYBRID_SEARCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    HYBRID_SEARCH_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è Hybrid search dependencies missing. Install with: pip install rank-bm25 nltk\")\n",
        "\n",
        "class BillAssistant:\n",
        "    \"\"\"\n",
        "    Class-based bill assistant with semantic Q&A using sentence-transformer embeddings.\n",
        "\n",
        "    Usage:\n",
        "        assistant = BillAssistant(model_name='deepseek-ai/DeepSeek-OCR')\n",
        "        assistant.run_cli()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"deepseek-ai/DeepSeek-OCR\", use_colab_secrets: bool = False):\n",
        "        self.model_name = model_name\n",
        "        self.device_info = self._gather_device_info()\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.sentence_model = None\n",
        "        self.client = None  # Groq client (optional)\n",
        "        self.current_bill = None\n",
        "        self.bill_text = None\n",
        "        self.pdf_support = PDF_SUPPORT\n",
        "\n",
        "        # Semantic structures\n",
        "        self.chunks = []               # list[str]\n",
        "        self.chunk_embeddings = None   # numpy.ndarray shape (n_chunks, emb_dim)\n",
        "        self.bill_embeddings = None    # embedding of whole bill (optional)\n",
        "\n",
        "        # Hybrid search components\n",
        "        self.bm25 = None\n",
        "        self.bm25_corpus = None\n",
        "        self.hybrid_search_available = HYBRID_SEARCH_AVAILABLE\n",
        "\n",
        "        # Optionally load Colab secrets and Groq client\n",
        "        if use_colab_secrets and colab_secrets_available and Groq is not None:\n",
        "            try:\n",
        "                GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\")\n",
        "                if GROQ_API_KEY:\n",
        "                    self.client = Groq(api_key=GROQ_API_KEY)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Failed to init Colab secrets / Groq client: {e}\")\n",
        "\n",
        "        # Load models eagerly (you may want to lazy-load in heavy environments)\n",
        "        self.load_models()\n",
        "\n",
        "    # ------ Utility / device info ------\n",
        "    def _gather_device_info(self):\n",
        "        try:\n",
        "            cuda_available = torch.cuda.is_available()\n",
        "            if cuda_available:\n",
        "                try:\n",
        "                    device_name = torch.cuda.get_device_name(0)\n",
        "                except Exception:\n",
        "                    device_name = \"Unknown CUDA device\"\n",
        "                try:\n",
        "                    compute_cap = torch.cuda.get_device_capability(0)\n",
        "                except Exception:\n",
        "                    compute_cap = (\"N/A\",)\n",
        "            else:\n",
        "                device_name = \"CPU\"\n",
        "                compute_cap = (\"N/A\",)\n",
        "            return {\n",
        "                \"torch_version\": torch.__version__,\n",
        "                \"cuda_available\": cuda_available,\n",
        "                \"device_name\": device_name,\n",
        "                \"compute_capability\": compute_cap,\n",
        "                \"flash_attn\": getattr(flash_attn, \"__version__\", None) if flash_attn else None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def print_device_info(self):\n",
        "        info = self.device_info\n",
        "        print(f\"PyTorch version: {info.get('torch_version')}\")\n",
        "        print(f\"CUDA available: {info.get('cuda_available')}\")\n",
        "        print(f\"GPU: {info.get('device_name')}\")\n",
        "        print(f\"Compute capability: {info.get('compute_capability')}\")\n",
        "        if info.get(\"flash_attn\"):\n",
        "            print(f\"‚úì Flash Attention version: {info.get('flash_attn')}\")\n",
        "        else:\n",
        "            print(\"‚úó Flash Attention not installed or not available\")\n",
        "\n",
        "    # ------ Model loading ------\n",
        "    def load_models(self):\n",
        "        \"Load tokenizer, model and sentence-transformer used for embeddings.\"\n",
        "        print(\"Loading Assistant Eye...\", end='\\t')\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
        "            self.model = AutoModel.from_pretrained(\n",
        "                self.model_name,\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                use_safetensors=True\n",
        "            )\n",
        "            self.model = self.model.eval()\n",
        "            print(\"‚úÖ ASSISTANT EYE LOADED SUCCESSFULLY!!!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to load model/tokenizer: {e}\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "        try:\n",
        "            print(\"Loading Assistant Brain...\", end='\\t')\n",
        "            self.sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "            print(\"‚úÖ ASSISTANT BRAIN LOADED SUCCESSFULLY!!!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to load sentence-transformer: {e}\")\n",
        "            self.sentence_model = None\n",
        "\n",
        "    # ------ Core OCR / inference run ------\n",
        "    def model_run(self, prompt: str, image_file: str):\n",
        "        \"\"\"\n",
        "        Run the OCR/inference model.\n",
        "        - If the real model is available, call model.infer(...) as per original script.\n",
        "        - If not available (or for debugging), returns a hardcoded sample result.\n",
        "        \"\"\"\n",
        "        output_path = f\"/content/outputs/{os.path.splitext(os.path.basename(image_file))[0]}\"\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            # fallback -- return debug sample text (same as your test)\n",
        "            print(\"‚ö†Ô∏è Model/tokenizer unavailable.\")\n",
        "            return \"\"\n",
        "\n",
        "        # If real model exists, call its inference method (kept as in original code)\n",
        "        try:\n",
        "            torch.cuda.empty_cache()\n",
        "            res = self.model.infer(\n",
        "                self.tokenizer,\n",
        "                prompt=prompt,\n",
        "                image_file=image_file,\n",
        "                output_path=output_path,\n",
        "                base_size=1536,\n",
        "                image_size=1024,\n",
        "                crop_mode=False,\n",
        "                save_results=True,\n",
        "                test_compress=False,\n",
        "                eval_mode=True,  # return instead of printing\n",
        "            )\n",
        "            print(\"Extractin complete.\")\n",
        "            return res\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Model inference failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def chunk_text(self, text: str, chunk_size: int = 400, overlap: int = 50):\n",
        "        \"\"\"\n",
        "        Split text into overlapping chunks (approx. chunk_size tokens/characters).\n",
        "        This uses naive character-based splitting for simplicity. For production,\n",
        "        use token-based splitting (e.g., tiktoken) to respect model token counts.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "        text = text.strip()\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        length = len(text)\n",
        "        while start < length:\n",
        "            end = start + chunk_size\n",
        "            chunk = text[start:end].strip()\n",
        "            if chunk:\n",
        "                chunks.append(chunk)\n",
        "            start = end - overlap  # overlap\n",
        "        return chunks\n",
        "\n",
        "    def compute_chunk_embeddings(self):\n",
        "        \"\"\"\n",
        "        Compute embeddings for each chunk and also store embedding for whole bill.\n",
        "        \"\"\"\n",
        "        if self.sentence_model is None:\n",
        "            print(\"‚ö†Ô∏è Sentence model not available; cannot compute embeddings.\")\n",
        "            self.chunk_embeddings = None\n",
        "            self.bill_embeddings = None\n",
        "            self.bm25 = None\n",
        "            self.bm25_corpus = None\n",
        "            return\n",
        "\n",
        "        if not self.chunks:\n",
        "            self.chunk_embeddings = None\n",
        "            self.bill_embeddings = None\n",
        "            self.bm25 = None\n",
        "            self.bm25_corpus = None\n",
        "            return\n",
        "\n",
        "        print(\"üß† Computing semantic embeddings...\")\n",
        "        emb = self.sentence_model.encode(self.chunks, convert_to_numpy=True)\n",
        "        # Normalize embeddings (helps cosine similarity)\n",
        "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1e-10\n",
        "        emb_norm = emb / norms\n",
        "        self.chunk_embeddings = emb_norm  # shape (n_chunks, d)\n",
        "\n",
        "        # whole-bill embedding\n",
        "        whole_emb = self.sentence_model.encode([self.bill_text], convert_to_numpy=True)\n",
        "        whole_emb /= (np.linalg.norm(whole_emb, axis=1, keepdims=True) + 1e-10)\n",
        "        self.bill_embeddings = whole_emb[0]\n",
        "\n",
        "       # Initialize BM25 for hybrid search if available\n",
        "        if self.hybrid_search_available:\n",
        "            print(\"üìö Building BM25 index for hybrid search...\")\n",
        "            try:\n",
        "                self.bm25_corpus = [self._preprocess_text(chunk) for chunk in self.chunks]\n",
        "                self.bm25 = BM25Okapi(self.bm25_corpus)\n",
        "                print(f\"‚úÖ BM25 index built with {len(self.chunks)} chunks!\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Failed to build BM25 index: {e}\")\n",
        "                self.bm25 = None\n",
        "                self.bm25_corpus = None\n",
        "        else:\n",
        "            self.bm25 = None\n",
        "            self.bm25_corpus = None\n",
        "\n",
        "    # ------ High-level processing ------\n",
        "    def process_bill(self, image_path: str, prompt: str = \"<image>\\nStrict OCR. Extract all the text in the image as Markdown.\"):\n",
        "        \"\"\"\n",
        "        Process a bill image or PDF: run OCR, parse to structured JSON, and compute embeddings.\n",
        "        Returns a status message (string).\n",
        "        \"\"\"\n",
        "        if not image_path:\n",
        "            return \"‚ùå No image path provided.\"\n",
        "\n",
        "        # If URL, download\n",
        "        if image_path.startswith(\"http\"):\n",
        "            try:\n",
        "                response = requests.get(image_path)\n",
        "                img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                tmp_path = \"/content/tmp/bill_download\"\n",
        "                _, ext = os.path.splitext(image_path)\n",
        "                tmp_path += ext.lower() if ext else \".jpg\"\n",
        "                img.save(tmp_path)\n",
        "                image_path = tmp_path\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå Failed to download image: {e}\"\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            return \"‚ùå File not found!\"\n",
        "\n",
        "        # Handle PDF files\n",
        "        if image_path.lower().endswith('.pdf'):\n",
        "            if not self.pdf_support:\n",
        "                return \"‚ùå PDF processing not available. Install PyMuPDF with: pip install PyMuPDF\"\n",
        "\n",
        "            print(\"üìÑ Processing PDF file (converting pages to images)...\")\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "            image_paths = []\n",
        "            try:\n",
        "                # Convert PDF to images\n",
        "                doc = fitz.open(image_path)\n",
        "                for page_num in range(doc.page_count):\n",
        "                    page = doc[page_num]\n",
        "                    pix = page.get_pixmap(dpi=150)  # 150 DPI for good quality\n",
        "                    img_path = os.path.join(temp_dir, f\"page_{page_num+1}.png\")\n",
        "                    pix.save(img_path)\n",
        "                    image_paths.append(img_path)\n",
        "                doc.close()\n",
        "\n",
        "                # Process each page\n",
        "                bill_texts = []\n",
        "                for i, img_path in enumerate(image_paths):\n",
        "                    print(f\"üìù Extracting text from page {i+1}/{len(image_paths)}...\")\n",
        "                    page_text = self.model_run(prompt, img_path)\n",
        "                    if not page_text.strip():\n",
        "                        page_text = \"[No text extracted from this page]\"\n",
        "                    bill_texts.append(f\"--- Page {i+1} ---\\n{page_text}\")\n",
        "\n",
        "                bill_text = \"\\n\\n\".join(bill_texts)\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå PDF processing failed: {e}\"\n",
        "            finally:\n",
        "                # Clean up temporary images\n",
        "                for img_path in image_paths:\n",
        "                    try:\n",
        "                        os.remove(img_path)\n",
        "                    except:\n",
        "                        pass\n",
        "                try:\n",
        "                    os.rmdir(temp_dir)\n",
        "                except:\n",
        "                    pass\n",
        "        else:\n",
        "            # Process single image\n",
        "            print(\"üìù Extracting text...\")\n",
        "            bill_text = self.model_run(prompt, image_path)\n",
        "\n",
        "        if not bill_text or not bill_text.strip():\n",
        "            return \"‚ùå No text extracted!\"\n",
        "\n",
        "        print(\"üß† Parsing with AI...\")\n",
        "        parsed_data = self.parse_bill_with_llm(bill_text)\n",
        "        if not parsed_data:\n",
        "            return \"‚ùå Parsing failed!\"\n",
        "\n",
        "        self.current_bill = parsed_data\n",
        "        self.bill_text = bill_text\n",
        "\n",
        "        # chunk & compute embeddings (semantic Q&A)\n",
        "        print(\"üîç Creating text chunks...\")\n",
        "        self.chunks = self.chunk_text(bill_text, chunk_size=400, overlap=50)\n",
        "        if not self.chunks:\n",
        "            self.chunks = [bill_text]\n",
        "\n",
        "        print(f\"üî¢ {len(self.chunks)} chunks created. Computing embeddings...\")\n",
        "        self.compute_chunk_embeddings()\n",
        "\n",
        "        return \"‚úÖ Bill processed successfully!\"\n",
        "\n",
        "    def _preprocess_text(self, text: str):\n",
        "        \"\"\"Preprocess text for BM25: lowercase, remove punctuation, remove stopwords.\"\"\"\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        tokens = text.split()\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
        "        return tokens\n",
        "\n",
        "    def _cosine_sim(self, a: np.ndarray, b: np.ndarray):\n",
        "        \"Compute cosine similarity between 1D a and 2D b (b is list of vectors).\"\n",
        "        if a.ndim == 1:\n",
        "            a = a.reshape(1, -1)\n",
        "        a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-10)\n",
        "        b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-10)\n",
        "        return np.dot(a_norm, b_norm.T).squeeze(0)  # shape (n_b,)\n",
        "\n",
        "    def semantic_search(self, query: str, top_k: int = 3, alpha: float = 0.7):\n",
        "        \"\"\"\n",
        "        Hybrid search combining BM25 (keyword) and semantic (embedding) scores.\n",
        "        alpha: weight for semantic similarity (0.0 = BM25 only, 1.0 = semantic only)\n",
        "        Returns list of tuples: (chunk_text, combined_score, chunk_index)\n",
        "        \"\"\"\n",
        "        if not self.chunks:\n",
        "            return []\n",
        "\n",
        "        # Always compute semantic scores if available\n",
        "        semantic_scores = np.zeros(len(self.chunks))\n",
        "        if self.chunk_embeddings is not None and self.sentence_model is not None:\n",
        "            try:\n",
        "                q_emb = self.sentence_model.encode([query], convert_to_numpy=True)[0]\n",
        "                q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-10)\n",
        "                semantic_scores = self._cosine_sim(q_emb, self.chunk_embeddings)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Semantic search failed: {e}\")\n",
        "\n",
        "        # Compute BM25 scores if available\n",
        "        bm25_scores = np.zeros(len(self.chunks))\n",
        "        if self.bm25 is not None:\n",
        "            try:\n",
        "                query_tokens = self._preprocess_text(query)\n",
        "                bm25_scores = self.bm25.get_scores(query_tokens)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è BM25 search failed: {e}\")\n",
        "\n",
        "        # Normalize scores to [0, 1] range\n",
        "        def normalize(scores):\n",
        "            min_score = np.min(scores)\n",
        "            max_score = np.max(scores)\n",
        "            if max_score - min_score < 1e-10:  # Avoid division by zero\n",
        "                return scores\n",
        "            return (scores - min_score) / (max_score - min_score + 1e-10)\n",
        "\n",
        "        norm_semantic = normalize(semantic_scores)\n",
        "        norm_bm25 = normalize(bm25_scores)\n",
        "\n",
        "        # Combine scores with weight alpha\n",
        "        combined_scores = alpha * norm_semantic + (1 - alpha) * norm_bm25\n",
        "\n",
        "        # Get top indices (descending order)\n",
        "        top_idx = np.argsort(-combined_scores)[:top_k]\n",
        "        results = [(self.chunks[i], float(combined_scores[i]), int(i)) for i in top_idx]\n",
        "        return results\n",
        "\n",
        "\n",
        "    def _clean_json(self, text: str) -> str:\n",
        "      if text is None:\n",
        "          return \"\"\n",
        "      text = text.replace('\\\\r\\\\n', '\\n').replace('\\\\n', '\\n').strip()\n",
        "      text = re.sub(r\"^```(?:json)?\\s*\\n?\", \"\", text.strip(), flags=re.IGNORECASE)\n",
        "      text = re.sub(r\"\\n?```$\", \"\", text, flags=re.IGNORECASE)\n",
        "      return text.strip()\n",
        "\n",
        "    def _basic_repair(self, text: str) -> str:\n",
        "      t = text\n",
        "      # common fixes (same idea as earlier)\n",
        "      t = re.sub(r'\"\\s+\"', r'\"', t)\n",
        "      t = re.sub(r'\"\\s+\"\"', r'\"\"', t)\n",
        "      t = re.sub(r':\\s*\"\\s*\"\\s*([^\"\\n\\r]+)\"', r': \"\\1\"', t)\n",
        "      t = re.sub(r':\\s*\"\\s*([^\"\\n\\r]+)\"', r': \"\\1\"', t)  # keep trying to remove stray quotes\n",
        "      t = re.sub(r'\":\\s*\"\\s+([^\"\\n\\r]+)\"', r'\": \"\\1\"', t)\n",
        "      t = re.sub(r\"\\'([^\\']*)\\'\", r'\"\\1\"', t)\n",
        "      t = re.sub(r',\\s*(\\}|\\])', r'\\1', t)\n",
        "      t = re.sub(r'\"\\s*\\}\\s*\\s*\\{', r'\"\\n},\\n{', t)\n",
        "      t = ''.join(ch for ch in t if ch == '\\n' or (31 < ord(ch) < 127))\n",
        "      t = re.sub(r'(?m)^(\\s*)([A-Za-z0-9_\\-]+)\\s*:', r'\\1\"\\2\":', t)\n",
        "      t = re.sub(r'\"\\s+([^\"]+?)\\s+\"', lambda m: f'\"{m.group(1).strip()}\"', t)\n",
        "      # don't auto-append braces/brackets here ‚Äî leave to the more aggressive routine\n",
        "      return t.strip()\n",
        "\n",
        "    def _balance_closers(self, s: str) -> str:\n",
        "      # Add minimal closers to match opens\n",
        "      open_braces = s.count('{')\n",
        "      close_braces = s.count('}')\n",
        "      open_brackets = s.count('[')\n",
        "      close_brackets = s.count(']')\n",
        "      if open_braces > close_braces:\n",
        "          s = s + ('}' * (open_braces - close_braces))\n",
        "      if open_brackets > close_brackets:\n",
        "          s = s + (']' * (open_brackets - close_brackets))\n",
        "      return s\n",
        "\n",
        "    def safe_load_json_recover(self, raw_text: str, debug: bool = False, max_trim_chars: int = 3000):\n",
        "      \"\"\"\n",
        "      Attempt to clean/repair LLM-produced JSON and recover a Python object.\n",
        "      Strategy:\n",
        "        1) Clean and do basic regex repairs.\n",
        "        2) Try json.loads.\n",
        "        3) If it fails, attempt balancing quotes/braces/brackets and try again.\n",
        "        4) If still fails, progressively trim from the end (up to `max_trim_chars`) and try balancing + loads.\n",
        "      Returns: Python object (dict/list) or raises ValueError with repaired snippet for inspection.\n",
        "      \"\"\"\n",
        "      cleaned = self._clean_json(raw_text)\n",
        "      repaired = self._basic_repair(cleaned)\n",
        "\n",
        "      if debug:\n",
        "          print(\"=== Initial repaired ===\")\n",
        "          print(repaired)\n",
        "          print(\"=== Trying json.loads ===\")\n",
        "\n",
        "      # 1) Try direct load after basic repair\n",
        "      try:\n",
        "          return json.loads(repaired)\n",
        "      except json.JSONDecodeError as e:\n",
        "          pass\n",
        "\n",
        "      # 2) If odd number of double-quotes, try closing the last quote\n",
        "      if repaired.count('\"') % 2 == 1:\n",
        "          cand = repaired + '\"'\n",
        "          try:\n",
        "              return json.loads(self._balance_closers(cand))\n",
        "          except Exception:\n",
        "              # continue to trimming attempts\n",
        "              pass\n",
        "\n",
        "      # 3) Try adding minimal closers and reloading\n",
        "      cand = self._balance_closers(repaired)\n",
        "      try:\n",
        "          return json.loads(cand)\n",
        "      except Exception:\n",
        "          pass\n",
        "\n",
        "      # 4) Progressive trimming: remove trailing characters (one by one or in small chunks)\n",
        "      #    and try to parse the prefix + balanced closers.\n",
        "      L = len(repaired)\n",
        "      # we'll try trimming up to max_trim_chars, in steps (bigger steps at first)\n",
        "      step = 1\n",
        "      trimmed = None\n",
        "      for trim in range(0, min(max_trim_chars, L), step):\n",
        "          if trim == 0:\n",
        "              candidate = repaired\n",
        "          else:\n",
        "              candidate = repaired[:L - trim]\n",
        "\n",
        "          # If candidate ends in a partial token like ' \"qty\": 3, \"amount\":', remove a trailing incomplete token:\n",
        "          # remove trailing sequences after the last '}' or ']' if they exist\n",
        "          last_close = max(candidate.rfind('}'), candidate.rfind(']'))\n",
        "          if last_close != -1 and last_close > len(candidate) - 200:\n",
        "              # keep up to last_close (close object/array)\n",
        "              candidate = candidate[:last_close+1]\n",
        "\n",
        "          # Close any open quotes (best-effort)\n",
        "          if candidate.count('\"') % 2 == 1:\n",
        "              candidate = candidate + '\"'\n",
        "\n",
        "          # Balance braces/brackets\n",
        "          candidate = self._balance_closers(candidate)\n",
        "\n",
        "          try:\n",
        "              parsed = json.loads(candidate)\n",
        "              if debug:\n",
        "                  print(f\"Recovered by trimming {trim} chars.\")\n",
        "              return parsed\n",
        "          except Exception:\n",
        "              # increase step size after initial few tries to speed up\n",
        "              if trim < 50:\n",
        "                  step = 1\n",
        "              elif trim < 200:\n",
        "                  step = 5\n",
        "              else:\n",
        "                  step = 20\n",
        "              continue\n",
        "\n",
        "      # If all attempts fail, raise with helpful diagnostic including the best-effort repaired text\n",
        "      # Provide truncated snippet to avoid enormous message\n",
        "      best_snippet = repaired[:4000] + (\"... (truncated)\" if len(repaired) > 4000 else \"\")\n",
        "      msg = (\n",
        "          \"Failed to parse JSON after attempted repairs and progressive trimming.\\n\\n\"\n",
        "          \"A best-effort repaired snippet is shown below (inspect to decide next action):\\n\\n\"\n",
        "          f\"{best_snippet}\\n\\n\"\n",
        "          \"Common fixes: ask the LLM to re-output only the JSON inside a single ```json``` codeblock, \"\n",
        "          \"increase the model `max_tokens` for longer outputs, or detect why the output was truncated.\"\n",
        "      )\n",
        "      raise ValueError(msg)\n",
        "\n",
        "    # ------ Parsing (LLM or hardcoded) ------\n",
        "    def parse_bill_with_llm(self, text: str):\n",
        "        \"Parse bill text into structured JSON.\"\n",
        "        # If you want to call Groq LLM, you might do something like:\n",
        "        if self.client is not None:\n",
        "            prompt = f\"\"\"\n",
        "                Extract structured data from this bill text as JSON:\n",
        "                {text}\n",
        "\n",
        "                Required fields:\n",
        "                - invoice_number, invoice_date, due_date, po_number\n",
        "                - bill_to (name, address, phone)\n",
        "                - ship_to (name, address, phone)\n",
        "                - items (qty, description, unit_price, amount)\n",
        "                - subtotal, tax, total\n",
        "                - company_name, company_address\n",
        "\n",
        "                Return ONLY valid JSON with these fields. Use empty strings for missing data.\n",
        "                \"\"\"\n",
        "            print(f\"\\n\\nPROMPT:\\n{prompt}\\n\\n\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        model=\"llama-3.1-8b-instant\",\n",
        "                        temperature=0.1,\n",
        "                        max_tokens=1024\n",
        "                    )\n",
        "            response_content = response.choices[0].message.content\n",
        "\n",
        "            try:\n",
        "                parsed = self.safe_load_json_recover(response_content, debug=True)\n",
        "                print(\"Parsed OK:\", parsed)\n",
        "                return parsed\n",
        "            except ValueError as e:\n",
        "                print(\"Could not parse. Inspect repaired output:\")\n",
        "                print(str(e))\n",
        "                return {}\n",
        "\n",
        "    # ---------------- answering queries ----------------\n",
        "    def answer_query(self, query: str, top_k: int = 3):\n",
        "        if not self.current_bill:\n",
        "            return \"‚ùå Process a bill first!\"\n",
        "\n",
        "        query_lower = query.lower()\n",
        "        if \"total\" in query_lower:\n",
        "            return f\"üí∞ Total: ${self.current_bill.get('total', 'N/A')}\"\n",
        "        if \"invoice number\" in query_lower or \"invoice #\" in query_lower or \"invoice\" == query_lower.strip():\n",
        "            return f\"üìã Invoice: {self.current_bill.get('invoice_number', 'N/A')}\"\n",
        "        if \"date\" in query_lower:\n",
        "            return f\"üìÖ Date: {self.current_bill.get('invoice_date', 'N/A')}\"\n",
        "        if \"items\" in query_lower:\n",
        "            items = self.current_bill.get(\"items\", [])\n",
        "            if items:\n",
        "                df = pd.DataFrame(items)\n",
        "                return f\"üõí Items:\\n{tabulate(df, headers='keys', tablefmt='grid')}\"\n",
        "            return \"‚ÑπÔ∏è No items found\"\n",
        "\n",
        "        # fallback to semantic retrieval\n",
        "        retrieved = self.semantic_search(query, top_k=top_k)\n",
        "        if not retrieved:\n",
        "            return \"‚ÑπÔ∏è No relevant content found in the bill.\"\n",
        "        # print(f\"\\nRETRIEVED DATA\\n{retrieved}\")\n",
        "\n",
        "        if self.client is not None:\n",
        "            context_text = \"\\n\\n---\\n\\n\".join([f\"Chunk {idx} (score {score:.4f}):\\n{text}\" for text, score, idx in retrieved])\n",
        "            prompt = (\n",
        "                f\"Use ONLY the context below (do NOT hallucinate). \"\n",
        "                f\"Extract an answer to the question. If information is not present, say 'Not found'.\\n\\n\"\n",
        "                f\"CONTEXT:\\n{context_text}\\n\\n\"\n",
        "                f\"QUESTION: {query}\\n\\n\"\n",
        "                f\"Answer concisely based ONLY on the context above:\"\n",
        "            )\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    model=\"llama-3.1-8b-instant\",\n",
        "                    temperature=0.0,\n",
        "                    max_tokens=256\n",
        "                )\n",
        "                return response.choices[0].message.content\n",
        "            except Exception as e:\n",
        "                # fallback to returning chunks\n",
        "                return f\"‚ö†Ô∏è LLM query failed: {e}\\n\\nTop relevant text:\\n\\n\" + \"\\n\\n---\\n\\n\".join([t for t, s, i in retrieved])\n",
        "\n",
        "        # If no LLM, return the top chunks concatenated with scores\n",
        "        best_text = \"\\n\\n---\\n\\n\".join([f\"(score {s:.4f})\\n{t}\" for t, s, i in retrieved])\n",
        "        return f\"üîé Top relevant bill text (best {len(retrieved)} chunks):\\n\\n{best_text}\"\n",
        "\n",
        "    # ------ Export ------\n",
        "    def export_to_json(self, filename: str = None):\n",
        "        \"Export the current bill to a JSON file.\"\n",
        "        if not self.current_bill:\n",
        "            return \"‚ùå No bill data available!\"\n",
        "\n",
        "        if not filename:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"bill_{timestamp}.json\"\n",
        "\n",
        "        try:\n",
        "            with open(filename, \"w\") as f:\n",
        "                json.dump(self.current_bill, f, indent=2)\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Failed to export: {e}\"\n",
        "        return f\"‚úÖ Exported to {filename}\"\n",
        "\n",
        "    # ------ Chat ------\n",
        "    def chat(self, message: str):\n",
        "        \"Simple chat handler. Uses Groq if available for richer replies.\"\n",
        "        msg_lower = message.lower().strip()\n",
        "\n",
        "        if msg_lower in (\"hi\", \"hello\", \"hey\"):\n",
        "            return \"üëã Hello! I'm your bill assistant. I can help you extract data from bills, answer questions, and export information.\"\n",
        "\n",
        "        if \"help\" in msg_lower:\n",
        "            help_text = \"üí° I can:\\n1. Process bill images and PDFs\\n2. Answer questions about bills\\n3. Export data to JSON\\n4. General chat\"\n",
        "            if self.current_bill:\n",
        "                help_text += \"\\n\\n‚úÖ Current bill loaded! Ask about totals, dates, items, or custom questions.\"\n",
        "            else:\n",
        "                help_text += \"\\n\\n‚ö†Ô∏è No bill processed yet. Please process a bill first to ask questions about it.\"\n",
        "            return help_text\n",
        "\n",
        "        # Handle queries when bill is loaded\n",
        "        if self.current_bill is not None:\n",
        "            return self.answer_query(message, top_k=3)\n",
        "\n",
        "        # No bill loaded - provide helpful guidance without LLM calls\n",
        "        if any(keyword in msg_lower for keyword in [\"bill\", \"invoice\", \"receipt\", \"document\", \"pdf\", \"image\", \"process\", \"upload\", \"load\", \"scan\"]):\n",
        "            return \"üìé Please process a bill first using option 1 in the main menu. I can handle images and PDFs!\"\n",
        "\n",
        "        if any(keyword in msg_lower for keyword in [\"thank\", \"bye\", \"exit\", \"goodbye\"]):\n",
        "            return \"üëã You can exit chat mode anytime by typing 'exit' or 'quit'.\"\n",
        "\n",
        "        # General fallback when no bill is loaded\n",
        "        return \"ü§ñ I'm ready to help with bills! Please process a bill first (option 1), then ask questions like 'What's the total?' or 'Show items'. Type 'help' for options.\"\n",
        "\n",
        "    def chat_loop(self):\n",
        "        \"Continuous chat loop until user exits.\"\n",
        "        print(\"\\nüí¨ Chat mode activated (type 'exit' or 'quit' to return to main menu)\")\n",
        "        print(\"ü§ñ Assistant: Hello! I'm your bill assistant. How can I help you today?\")\n",
        "\n",
        "        while True:\n",
        "            message = input(\"You: \").strip()\n",
        "\n",
        "            if message.lower() in ('exit', 'quit', 'bye', 'goodbye'):\n",
        "                print(\"üëã Exiting chat mode...\")\n",
        "                break\n",
        "\n",
        "            response = self.chat(message)\n",
        "            print(f\"ü§ñ Assistant: {response}\")\n",
        "\n",
        "    def run_cli(self):\n",
        "        \"\"\"Interactive command-line menu.\"\"\"\n",
        "        self.print_device_info()\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"BILL ASSISTANT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        print(\"\\n1. üì∏ Process bill image/PDF\")\n",
        "        print(\"2. üíæ Export to JSON\")\n",
        "        print(\"3. üí¨ Chat\")\n",
        "        print(\"4. üö™ Exit\")\n",
        "\n",
        "        while True:\n",
        "            choice = input(\"\\nSelect option (1-4): \").strip()\n",
        "\n",
        "            if choice == \"1\":\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                img_path = input(\"Enter image/PDF path (or URL): \").strip()\n",
        "                result = self.process_bill(img_path)\n",
        "                print(result)\n",
        "                if self.current_bill:\n",
        "                    print(\"\\nüìä Bill Summary:\")\n",
        "                    print(f\"Invoice: {self.current_bill.get('invoice_number', 'N/A')}\")\n",
        "                    print(f\"Date: {self.current_bill.get('invoice_date', 'N/A')}\")\n",
        "                    print(f\"Total: ${self.current_bill.get('total', 'N/A')}\")\n",
        "                print(\"\\n\" + \"=\" * 50)\n",
        "                print(\"BILL ASSISTANT\")\n",
        "                print(\"=\" * 50)\n",
        "                print(\"\\n1. üì∏ Process bill image/PDF\")\n",
        "                print(\"2. üíæ Export to JSON\")\n",
        "                print(\"3. üí¨ Chat\")\n",
        "                print(\"4. üö™ Exit\")\n",
        "\n",
        "            elif choice == \"2\":\n",
        "                filename = input(\"Enter filename (or press Enter for default): \").strip()\n",
        "                result = self.export_to_json(filename if filename else None)\n",
        "                print(result)\n",
        "\n",
        "            elif choice == \"3\":\n",
        "                self.chat_loop()  # Call the new continuous chat loop\n",
        "                # Show menu again after exiting chat\n",
        "                print(\"\\n\" + \"=\" * 50)\n",
        "                print(\"BILL ASSISTANT\")\n",
        "                print(\"=\" * 50)\n",
        "                print(\"\\n1. üì∏ Process bill image/PDF\")\n",
        "                print(\"2. üíæ Export to JSON\")\n",
        "                print(\"3. üí¨ Chat\")\n",
        "                print(\"4. üö™ Exit\")\n",
        "\n",
        "            elif choice == \"4\":\n",
        "                print(\"üëã Goodbye!\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"‚ùå Invalid choice!\")\n",
        "\n",
        "# If run as script, start the CLI3\n",
        "if __name__ == \"__main__\":\n",
        "    assistant = BillAssistant(use_colab_secrets=True)\n",
        "    assistant.run_cli()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deploy the application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LimzbjWe6iy",
        "outputId": "afd85f31-fe3d-4aed-e8d4-4c092befd46e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing assistant.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile assistant.py\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "import tempfile  # For PDF processing\n",
        "\n",
        "# Third-party libs\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "from transformers import AutoModel, AutoTokenizer, logging as hf_logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Aux libs used in CLI\n",
        "import requests\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from tabulate import tabulate\n",
        "\n",
        "import flash_attn  # optional\n",
        "\n",
        "# Basic environment / warning control\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "warnings.filterwarnings(\"ignore\", message=\".*GetPrototype.*\")\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "from google.colab import userdata\n",
        "colab_secrets_available = True\n",
        "from groq import Groq\n",
        "import fitz  # PyMuPDF\n",
        "PDF_SUPPORT = True\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords', quiet=True)\n",
        "HYBRID_SEARCH_AVAILABLE = True\n",
        "\n",
        "class BillAssistant:\n",
        "    \"\"\"\n",
        "    Class-based bill assistant with semantic Q&A using sentence-transformer embeddings.\n",
        "\n",
        "    Usage:\n",
        "        assistant = BillAssistant(model_name='deepseek-ai/DeepSeek-OCR')\n",
        "        assistant.run_cli()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"deepseek-ai/DeepSeek-OCR\", use_colab_secrets: bool = False):\n",
        "        self.model_name = model_name\n",
        "        self.device_info = self._gather_device_info()\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.sentence_model = None\n",
        "        self.client = None  # Groq client (optional)\n",
        "        self.current_bill = None\n",
        "        self.bill_text = None\n",
        "        self.pdf_support = PDF_SUPPORT\n",
        "\n",
        "        # Semantic structures\n",
        "        self.chunks = []               # list[str]\n",
        "        self.chunk_embeddings = None   # numpy.ndarray shape (n_chunks, emb_dim)\n",
        "        self.bill_embeddings = None    # embedding of whole bill (optional)\n",
        "\n",
        "        # Hybrid search components\n",
        "        self.bm25 = None\n",
        "        self.bm25_corpus = None\n",
        "        self.hybrid_search_available = HYBRID_SEARCH_AVAILABLE\n",
        "\n",
        "        # Optionally load Colab secrets and Groq client\n",
        "        if use_colab_secrets and colab_secrets_available and Groq is not None:\n",
        "            try:\n",
        "                GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\")\n",
        "                if GROQ_API_KEY:\n",
        "                    self.client = Groq(api_key=GROQ_API_KEY)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Failed to init Colab secrets / Groq client: {e}\")\n",
        "\n",
        "        # Load models eagerly (you may want to lazy-load in heavy environments)\n",
        "        self.load_models()\n",
        "        self.print_device_info()\n",
        "\n",
        "    # ------ Utility / device info ------\n",
        "    def _gather_device_info(self):\n",
        "        try:\n",
        "            cuda_available = torch.cuda.is_available()\n",
        "            if cuda_available:\n",
        "                try:\n",
        "                    device_name = torch.cuda.get_device_name(0)\n",
        "                except Exception:\n",
        "                    device_name = \"Unknown CUDA device\"\n",
        "                try:\n",
        "                    compute_cap = torch.cuda.get_device_capability(0)\n",
        "                except Exception:\n",
        "                    compute_cap = (\"N/A\",)\n",
        "            else:\n",
        "                device_name = \"CPU\"\n",
        "                compute_cap = (\"N/A\",)\n",
        "            return {\n",
        "                \"torch_version\": torch.__version__,\n",
        "                \"cuda_available\": cuda_available,\n",
        "                \"device_name\": device_name,\n",
        "                \"compute_capability\": compute_cap,\n",
        "                \"flash_attn\": getattr(flash_attn, \"__version__\", None) if flash_attn else None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def print_device_info(self):\n",
        "        info = self.device_info\n",
        "        print(f\"PyTorch version: {info.get('torch_version')}\")\n",
        "        print(f\"CUDA available: {info.get('cuda_available')}\")\n",
        "        print(f\"GPU: {info.get('device_name')}\")\n",
        "        print(f\"Compute capability: {info.get('compute_capability')}\")\n",
        "        if info.get(\"flash_attn\"):\n",
        "            print(f\"‚úì Flash Attention version: {info.get('flash_attn')}\")\n",
        "        else:\n",
        "            print(\"‚úó Flash Attention not installed or not available\")\n",
        "\n",
        "    # ------ Model loading ------\n",
        "    def load_models(self):\n",
        "        \"Load tokenizer, model and sentence-transformer used for embeddings.\"\n",
        "        print(\"Loading Assistant Eye...\", end='\\t')\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
        "            self.model = AutoModel.from_pretrained(\n",
        "                self.model_name,\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                use_safetensors=True\n",
        "            )\n",
        "            self.model = self.model.eval()\n",
        "            print(\"‚úÖ ASSISTANT EYE LOADED SUCCESSFULLY!!!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to load model/tokenizer: {e}\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "        try:\n",
        "            print(\"Loading Assistant Brain...\", end='\\t')\n",
        "            self.sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "            print(\"‚úÖ ASSISTANT BRAIN LOADED SUCCESSFULLY!!!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to load sentence-transformer: {e}\")\n",
        "            self.sentence_model = None\n",
        "\n",
        "    # ------ Core OCR / inference run ------\n",
        "    def model_run(self, prompt: str, image_file: str):\n",
        "        \"\"\"\n",
        "        Run the OCR/inference model.\n",
        "        - If the real model is available, call model.infer(...) as per original script.\n",
        "        - If not available (or for debugging), returns a hardcoded sample result.\n",
        "        \"\"\"\n",
        "        output_path = f\"/content/outputs/{os.path.splitext(os.path.basename(image_file))[0]}\"\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            # fallback -- return debug sample text (same as your test)\n",
        "            print(\"‚ö†Ô∏è Model/tokenizer unavailable.\")\n",
        "            return \"\"\n",
        "\n",
        "        # If real model exists, call its inference method (kept as in original code)\n",
        "        try:\n",
        "            print(\"OCR...\")\n",
        "            torch.cuda.empty_cache()\n",
        "            res = self.model.infer(\n",
        "                self.tokenizer,\n",
        "                prompt=prompt,\n",
        "                image_file=image_file,\n",
        "                output_path=output_path,\n",
        "                base_size=1536,\n",
        "                image_size=1024,\n",
        "                crop_mode=False,\n",
        "                save_results=True,\n",
        "                test_compress=False,\n",
        "                eval_mode=True,  # return instead of printing\n",
        "            )\n",
        "            print(f\"Extraction complete.\\n{res}\")\n",
        "            return res\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Model inference failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def chunk_text(self, text: str, chunk_size: int = 400, overlap: int = 50):\n",
        "        \"\"\"\n",
        "        Split text into overlapping chunks (approx. chunk_size tokens/characters).\n",
        "        This uses naive character-based splitting for simplicity. For production,\n",
        "        use token-based splitting (e.g., tiktoken) to respect model token counts.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "        text = text.strip()\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        length = len(text)\n",
        "        while start < length:\n",
        "            end = start + chunk_size\n",
        "            chunk = text[start:end].strip()\n",
        "            if chunk:\n",
        "                chunks.append(chunk)\n",
        "            start = end - overlap  # overlap\n",
        "        return chunks\n",
        "\n",
        "    def compute_chunk_embeddings(self):\n",
        "        \"\"\"\n",
        "        Compute embeddings for each chunk and also store embedding for whole bill.\n",
        "        \"\"\"\n",
        "        if self.sentence_model is None:\n",
        "            print(\"‚ö†Ô∏è Sentence model not available; cannot compute embeddings.\")\n",
        "            self.chunk_embeddings = None\n",
        "            self.bill_embeddings = None\n",
        "            self.bm25 = None\n",
        "            self.bm25_corpus = None\n",
        "            return\n",
        "\n",
        "        if not self.chunks:\n",
        "            self.chunk_embeddings = None\n",
        "            self.bill_embeddings = None\n",
        "            self.bm25 = None\n",
        "            self.bm25_corpus = None\n",
        "            return\n",
        "\n",
        "        print(\"üß† Computing semantic embeddings...\")\n",
        "        emb = self.sentence_model.encode(self.chunks, convert_to_numpy=True)\n",
        "        # Normalize embeddings (helps cosine similarity)\n",
        "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1e-10\n",
        "        emb_norm = emb / norms\n",
        "        self.chunk_embeddings = emb_norm  # shape (n_chunks, d)\n",
        "\n",
        "        # whole-bill embedding\n",
        "        whole_emb = self.sentence_model.encode([self.bill_text], convert_to_numpy=True)\n",
        "        whole_emb /= (np.linalg.norm(whole_emb, axis=1, keepdims=True) + 1e-10)\n",
        "        self.bill_embeddings = whole_emb[0]\n",
        "\n",
        "       # Initialize BM25 for hybrid search if available\n",
        "        if self.hybrid_search_available:\n",
        "            print(\"üìö Building BM25 index for hybrid search...\")\n",
        "            try:\n",
        "                self.bm25_corpus = [self._preprocess_text(chunk) for chunk in self.chunks]\n",
        "                self.bm25 = BM25Okapi(self.bm25_corpus)\n",
        "                print(f\"‚úÖ BM25 index built with {len(self.chunks)} chunks!\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Failed to build BM25 index: {e}\")\n",
        "                self.bm25 = None\n",
        "                self.bm25_corpus = None\n",
        "        else:\n",
        "            self.bm25 = None\n",
        "            self.bm25_corpus = None\n",
        "\n",
        "    # ------ High-level processing ------\n",
        "    def process_bill(self, image_path: str, prompt: str = \"<image>\\nStrict OCR. Extract all the text in the image as Markdown.\"):\n",
        "        \"\"\"\n",
        "        Process a bill image or PDF: run OCR, parse to structured JSON, and compute embeddings.\n",
        "        Returns a status message (string).\n",
        "        \"\"\"\n",
        "        if not image_path:\n",
        "            return \"‚ùå No image path provided.\"\n",
        "\n",
        "        # If URL, download\n",
        "        if image_path.startswith(\"http\"):\n",
        "            try:\n",
        "                response = requests.get(image_path)\n",
        "                img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                tmp_path = \"/content/tmp/bill_download\"\n",
        "                _, ext = os.path.splitext(image_path)\n",
        "                tmp_path += ext.lower() if ext else \".jpg\"\n",
        "                img.save(tmp_path)\n",
        "                image_path = tmp_path\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå Failed to download image: {e}\"\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            return \"‚ùå File not found!\"\n",
        "\n",
        "        # Handle PDF files\n",
        "        if image_path.lower().endswith('.pdf'):\n",
        "            if not self.pdf_support:\n",
        "                return \"‚ùå PDF processing not available. Install PyMuPDF with: pip install PyMuPDF\"\n",
        "\n",
        "            print(\"üìÑ Processing PDF file (converting pages to images)...\")\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "            image_paths = []\n",
        "            try:\n",
        "                # Convert PDF to images\n",
        "                doc = fitz.open(image_path)\n",
        "                for page_num in range(doc.page_count):\n",
        "                    page = doc[page_num]\n",
        "                    pix = page.get_pixmap(dpi=150)  # 150 DPI for good quality\n",
        "                    img_path = os.path.join(temp_dir, f\"page_{page_num+1}.png\")\n",
        "                    pix.save(img_path)\n",
        "                    image_paths.append(img_path)\n",
        "                doc.close()\n",
        "\n",
        "                # Process each page\n",
        "                bill_texts = []\n",
        "                for i, img_path in enumerate(image_paths):\n",
        "                    print(f\"üìù Extracting text from page {i+1}/{len(image_paths)}...\")\n",
        "                    page_text = self.model_run(prompt, img_path)\n",
        "                    if not page_text.strip():\n",
        "                        page_text = \"[No text extracted from this page]\"\n",
        "                    bill_texts.append(f\"--- Page {i+1} ---\\n{page_text}\")\n",
        "\n",
        "                bill_text = \"\\n\\n\".join(bill_texts)\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå PDF processing failed: {e}\"\n",
        "            finally:\n",
        "                # Clean up temporary images\n",
        "                for img_path in image_paths:\n",
        "                    try:\n",
        "                        os.remove(img_path)\n",
        "                    except:\n",
        "                        pass\n",
        "                try:\n",
        "                    os.rmdir(temp_dir)\n",
        "                except:\n",
        "                    pass\n",
        "        else:\n",
        "            # Process single image\n",
        "            print(\"üìù Extracting text...\")\n",
        "            bill_text = self.model_run(prompt, image_path)\n",
        "\n",
        "        if not bill_text or not bill_text.strip():\n",
        "            return \"‚ùå No text extracted!\"\n",
        "\n",
        "        print(\"üß† Parsing with AI...\")\n",
        "        parsed_data = self.parse_bill_with_llm(bill_text)\n",
        "        print(f\"ParsedData:\\n{parsed_data}\")\n",
        "        if not parsed_data:\n",
        "            return \"‚ùå Parsing failed!\"\n",
        "\n",
        "        self.current_bill = parsed_data\n",
        "        self.bill_text = bill_text\n",
        "\n",
        "        # chunk & compute embeddings (semantic Q&A)\n",
        "        print(\"üîç Creating text chunks...\")\n",
        "        self.chunks = self.chunk_text(bill_text, chunk_size=400, overlap=50)\n",
        "        if not self.chunks:\n",
        "            self.chunks = [bill_text]\n",
        "\n",
        "        print(f\"üî¢ {len(self.chunks)} chunks created. Computing embeddings...\")\n",
        "        self.compute_chunk_embeddings()\n",
        "\n",
        "        return \"‚úÖ Bill processed successfully!\"\n",
        "\n",
        "    def _preprocess_text(self, text: str):\n",
        "        \"\"\"Preprocess text for BM25: lowercase, remove punctuation, remove stopwords.\"\"\"\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        tokens = text.split()\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
        "        return tokens\n",
        "\n",
        "    def _cosine_sim(self, a: np.ndarray, b: np.ndarray):\n",
        "        \"Compute cosine similarity between 1D a and 2D b (b is list of vectors).\"\n",
        "        if a.ndim == 1:\n",
        "            a = a.reshape(1, -1)\n",
        "        a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-10)\n",
        "        b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-10)\n",
        "        return np.dot(a_norm, b_norm.T).squeeze(0)  # shape (n_b,)\n",
        "\n",
        "    def semantic_search(self, query: str, top_k: int = 3, alpha: float = 0.7):\n",
        "        \"\"\"\n",
        "        Hybrid search combining BM25 (keyword) and semantic (embedding) scores.\n",
        "        alpha: weight for semantic similarity (0.0 = BM25 only, 1.0 = semantic only)\n",
        "        Returns list of tuples: (chunk_text, combined_score, chunk_index)\n",
        "        \"\"\"\n",
        "        if not self.chunks:\n",
        "            return []\n",
        "\n",
        "        # Always compute semantic scores if available\n",
        "        semantic_scores = np.zeros(len(self.chunks))\n",
        "        if self.chunk_embeddings is not None and self.sentence_model is not None:\n",
        "            try:\n",
        "                q_emb = self.sentence_model.encode([query], convert_to_numpy=True)[0]\n",
        "                q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-10)\n",
        "                semantic_scores = self._cosine_sim(q_emb, self.chunk_embeddings)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Semantic search failed: {e}\")\n",
        "\n",
        "        # Compute BM25 scores if available\n",
        "        bm25_scores = np.zeros(len(self.chunks))\n",
        "        if self.bm25 is not None:\n",
        "            try:\n",
        "                query_tokens = self._preprocess_text(query)\n",
        "                bm25_scores = self.bm25.get_scores(query_tokens)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è BM25 search failed: {e}\")\n",
        "\n",
        "        # Normalize scores to [0, 1] range\n",
        "        def normalize(scores):\n",
        "            min_score = np.min(scores)\n",
        "            max_score = np.max(scores)\n",
        "            if max_score - min_score < 1e-10:  # Avoid division by zero\n",
        "                return scores\n",
        "            return (scores - min_score) / (max_score - min_score + 1e-10)\n",
        "\n",
        "        norm_semantic = normalize(semantic_scores)\n",
        "        norm_bm25 = normalize(bm25_scores)\n",
        "\n",
        "        # Combine scores with weight alpha\n",
        "        combined_scores = alpha * norm_semantic + (1 - alpha) * norm_bm25\n",
        "\n",
        "        # Get top indices (descending order)\n",
        "        top_idx = np.argsort(-combined_scores)[:top_k]\n",
        "        results = [(self.chunks[i], float(combined_scores[i]), int(i)) for i in top_idx]\n",
        "        return results\n",
        "\n",
        "\n",
        "    def _clean_json(self, text: str) -> str:\n",
        "      if text is None:\n",
        "          return \"\"\n",
        "      text = text.replace('\\\\r\\\\n', '\\n').replace('\\\\n', '\\n').strip()\n",
        "      text = re.sub(r\"^```(?:json)?\\s*\\n?\", \"\", text.strip(), flags=re.IGNORECASE)\n",
        "      text = re.sub(r\"\\n?```$\", \"\", text, flags=re.IGNORECASE)\n",
        "      return text.strip()\n",
        "\n",
        "    def _basic_repair(self, text: str) -> str:\n",
        "      t = text\n",
        "      # common fixes (same idea as earlier)\n",
        "      t = re.sub(r'\"\\s+\"', r'\"', t)\n",
        "      t = re.sub(r'\"\\s+\"\"', r'\"\"', t)\n",
        "      t = re.sub(r':\\s*\"\\s*\"\\s*([^\"\\n\\r]+)\"', r': \"\\1\"', t)\n",
        "      t = re.sub(r':\\s*\"\\s*([^\"\\n\\r]+)\"', r': \"\\1\"', t)  # keep trying to remove stray quotes\n",
        "      t = re.sub(r'\":\\s*\"\\s+([^\"\\n\\r]+)\"', r'\": \"\\1\"', t)\n",
        "      t = re.sub(r\"\\'([^\\']*)\\'\", r'\"\\1\"', t)\n",
        "      t = re.sub(r',\\s*(\\}|\\])', r'\\1', t)\n",
        "      t = re.sub(r'\"\\s*\\}\\s*\\s*\\{', r'\"\\n},\\n{', t)\n",
        "      t = ''.join(ch for ch in t if ch == '\\n' or (31 < ord(ch) < 127))\n",
        "      t = re.sub(r'(?m)^(\\s*)([A-Za-z0-9_\\-]+)\\s*:', r'\\1\"\\2\":', t)\n",
        "      t = re.sub(r'\"\\s+([^\"]+?)\\s+\"', lambda m: f'\"{m.group(1).strip()}\"', t)\n",
        "      # don't auto-append braces/brackets here ‚Äî leave to the more aggressive routine\n",
        "      return t.strip()\n",
        "\n",
        "    def _balance_closers(self, s: str) -> str:\n",
        "      # Add minimal closers to match opens\n",
        "      open_braces = s.count('{')\n",
        "      close_braces = s.count('}')\n",
        "      open_brackets = s.count('[')\n",
        "      close_brackets = s.count(']')\n",
        "      if open_braces > close_braces:\n",
        "          s = s + ('}' * (open_braces - close_braces))\n",
        "      if open_brackets > close_brackets:\n",
        "          s = s + (']' * (open_brackets - close_brackets))\n",
        "      return s\n",
        "\n",
        "    def safe_load_json_recover(self, raw_text: str, debug: bool = False, max_trim_chars: int = 3000):\n",
        "      \"\"\"\n",
        "      Attempt to clean/repair LLM-produced JSON and recover a Python object.\n",
        "      Strategy:\n",
        "        1) Clean and do basic regex repairs.\n",
        "        2) Try json.loads.\n",
        "        3) If it fails, attempt balancing quotes/braces/brackets and try again.\n",
        "        4) If still fails, progressively trim from the end (up to `max_trim_chars`) and try balancing + loads.\n",
        "      Returns: Python object (dict/list) or raises ValueError with repaired snippet for inspection.\n",
        "      \"\"\"\n",
        "      cleaned = self._clean_json(raw_text)\n",
        "      repaired = self._basic_repair(cleaned)\n",
        "\n",
        "      if debug:\n",
        "          print(\"=== Initial repaired ===\")\n",
        "          print(repaired)\n",
        "          print(\"=== Trying json.loads ===\")\n",
        "\n",
        "      # 1) Try direct load after basic repair\n",
        "      try:\n",
        "          return json.loads(repaired)\n",
        "      except json.JSONDecodeError as e:\n",
        "          pass\n",
        "\n",
        "      # 2) If odd number of double-quotes, try closing the last quote\n",
        "      if repaired.count('\"') % 2 == 1:\n",
        "          cand = repaired + '\"'\n",
        "          try:\n",
        "              return json.loads(self._balance_closers(cand))\n",
        "          except Exception:\n",
        "              # continue to trimming attempts\n",
        "              pass\n",
        "\n",
        "      # 3) Try adding minimal closers and reloading\n",
        "      cand = self._balance_closers(repaired)\n",
        "      try:\n",
        "          return json.loads(cand)\n",
        "      except Exception:\n",
        "          pass\n",
        "\n",
        "      # 4) Progressive trimming: remove trailing characters (one by one or in small chunks)\n",
        "      #    and try to parse the prefix + balanced closers.\n",
        "      L = len(repaired)\n",
        "      # we'll try trimming up to max_trim_chars, in steps (bigger steps at first)\n",
        "      step = 1\n",
        "      trimmed = None\n",
        "      for trim in range(0, min(max_trim_chars, L), step):\n",
        "          if trim == 0:\n",
        "              candidate = repaired\n",
        "          else:\n",
        "              candidate = repaired[:L - trim]\n",
        "\n",
        "          # If candidate ends in a partial token like ' \"qty\": 3, \"amount\":', remove a trailing incomplete token:\n",
        "          # remove trailing sequences after the last '}' or ']' if they exist\n",
        "          last_close = max(candidate.rfind('}'), candidate.rfind(']'))\n",
        "          if last_close != -1 and last_close > len(candidate) - 200:\n",
        "              # keep up to last_close (close object/array)\n",
        "              candidate = candidate[:last_close+1]\n",
        "\n",
        "          # Close any open quotes (best-effort)\n",
        "          if candidate.count('\"') % 2 == 1:\n",
        "              candidate = candidate + '\"'\n",
        "\n",
        "          # Balance braces/brackets\n",
        "          candidate = self._balance_closers(candidate)\n",
        "\n",
        "          try:\n",
        "              parsed = json.loads(candidate)\n",
        "              if debug:\n",
        "                  print(f\"Recovered by trimming {trim} chars.\")\n",
        "              return parsed\n",
        "          except Exception:\n",
        "              # increase step size after initial few tries to speed up\n",
        "              if trim < 50:\n",
        "                  step = 1\n",
        "              elif trim < 200:\n",
        "                  step = 5\n",
        "              else:\n",
        "                  step = 20\n",
        "              continue\n",
        "\n",
        "      # If all attempts fail, raise with helpful diagnostic including the best-effort repaired text\n",
        "      # Provide truncated snippet to avoid enormous message\n",
        "      best_snippet = repaired[:4000] + (\"... (truncated)\" if len(repaired) > 4000 else \"\")\n",
        "      msg = (\n",
        "          \"Failed to parse JSON after attempted repairs and progressive trimming.\\n\\n\"\n",
        "          \"A best-effort repaired snippet is shown below (inspect to decide next action):\\n\\n\"\n",
        "          f\"{best_snippet}\\n\\n\"\n",
        "          \"Common fixes: ask the LLM to re-output only the JSON inside a single ```json``` codeblock, \"\n",
        "          \"increase the model `max_tokens` for longer outputs, or detect why the output was truncated.\"\n",
        "      )\n",
        "      raise ValueError(msg)\n",
        "\n",
        "    # ------ Parsing (LLM or hardcoded) ------\n",
        "    def parse_bill_with_llm(self, text: str):\n",
        "        \"Parse bill text into structured JSON.\"\n",
        "        print(f\"Parsing with LLM...\")\n",
        "        # If you want to call Groq LLM, you might do something like:\n",
        "        if self.client is not None:\n",
        "            print(f\"Cient found.\")\n",
        "            prompt = f\"\"\"\n",
        "                Extract structured data from this bill text as JSON:\n",
        "                {text}\n",
        "\n",
        "                Required fields:\n",
        "                - invoice_number, invoice_date, due_date, po_number\n",
        "                - bill_to (name, address, phone)\n",
        "                - ship_to (name, address, phone)\n",
        "                - items (qty, description, unit_price, amount)\n",
        "                - subtotal, tax, total\n",
        "                - company_name, company_address\n",
        "\n",
        "                Return ONLY valid JSON with these fields. Use empty strings for missing data.\n",
        "                \"\"\"\n",
        "            print(f\"\\n\\nPROMPT:\\n{prompt}\\n\\n\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        model=\"llama-3.1-8b-instant\",\n",
        "                        temperature=0.1,\n",
        "                        max_tokens=1024\n",
        "                    )\n",
        "            response_content = response.choices[0].message.content\n",
        "\n",
        "            try:\n",
        "                parsed = self.safe_load_json_recover(response_content, debug=True)\n",
        "                print(\"Parsed OK:\", parsed)\n",
        "                return parsed\n",
        "            except ValueError as e:\n",
        "                print(\"Could not parse. Inspect repaired output:\")\n",
        "                print(str(e))\n",
        "                return {}\n",
        "\n",
        "    # ---------------- answering queries ----------------\n",
        "    def answer_query(self, query: str, top_k: int = 3):\n",
        "        if not self.current_bill:\n",
        "            return \"‚ùå Process a bill first!\"\n",
        "\n",
        "        query_lower = query.lower()\n",
        "        if \"total\" in query_lower:\n",
        "            return f\"üí∞ Total: ${self.current_bill.get('total', 'N/A')}\"\n",
        "        if \"invoice number\" in query_lower or \"invoice #\" in query_lower or \"invoice\" == query_lower.strip():\n",
        "            return f\"üìã Invoice: {self.current_bill.get('invoice_number', 'N/A')}\"\n",
        "        if \"date\" in query_lower:\n",
        "            return f\"üìÖ Date: {self.current_bill.get('invoice_date', 'N/A')}\"\n",
        "        if \"items\" in query_lower:\n",
        "            items = self.current_bill.get(\"items\", [])\n",
        "            if items:\n",
        "                df = pd.DataFrame(items)\n",
        "                return f\"üõí Items:\\n{tabulate(df, headers='keys', tablefmt='grid')}\"\n",
        "            return \"‚ÑπÔ∏è No items found\"\n",
        "\n",
        "        # fallback to semantic retrieval\n",
        "        retrieved = self.semantic_search(query, top_k=top_k)\n",
        "        if not retrieved:\n",
        "            return \"‚ÑπÔ∏è No relevant content found in the bill.\"\n",
        "        # print(f\"\\nRETRIEVED DATA\\n{retrieved}\")\n",
        "\n",
        "        if self.client is not None:\n",
        "            context_text = \"\\n\\n---\\n\\n\".join([f\"Chunk {idx} (score {score:.4f}):\\n{text}\" for text, score, idx in retrieved])\n",
        "            prompt = (\n",
        "                f\"Use ONLY the context below (do NOT hallucinate). \"\n",
        "                f\"Extract an answer to the question. If information is not present, say 'Not found'.\\n\\n\"\n",
        "                f\"CONTEXT:\\n{context_text}\\n\\n\"\n",
        "                f\"QUESTION: {query}\\n\\n\"\n",
        "                f\"Answer concisely based ONLY on the context above:\"\n",
        "            )\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    model=\"llama-3.1-8b-instant\",\n",
        "                    temperature=0.0,\n",
        "                    max_tokens=256\n",
        "                )\n",
        "                return response.choices[0].message.content\n",
        "            except Exception as e:\n",
        "                # fallback to returning chunks\n",
        "                return f\"‚ö†Ô∏è LLM query failed: {e}\\n\\nTop relevant text:\\n\\n\" + \"\\n\\n---\\n\\n\".join([t for t, s, i in retrieved])\n",
        "\n",
        "        # If no LLM, return the top chunks concatenated with scores\n",
        "        best_text = \"\\n\\n---\\n\\n\".join([f\"(score {s:.4f})\\n{t}\" for t, s, i in retrieved])\n",
        "        return f\"üîé Top relevant bill text (best {len(retrieved)} chunks):\\n\\n{best_text}\"\n",
        "\n",
        "    # ------ Export ------\n",
        "    def export_to_json(self, filename: str = None):\n",
        "        \"Export the current bill to a JSON file.\"\n",
        "        if not self.current_bill:\n",
        "            return \"‚ùå No bill data available!\"\n",
        "\n",
        "        if not filename:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"bill_{timestamp}.json\"\n",
        "\n",
        "        try:\n",
        "            with open(filename, \"w\") as f:\n",
        "                json.dump(self.current_bill, f, indent=2)\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Failed to export: {e}\"\n",
        "        return f\"‚úÖ Exported to {filename}\"\n",
        "\n",
        "    # ------ Chat ------\n",
        "    def chat(self, message: str):\n",
        "        \"Simple chat handler. Uses Groq if available for richer replies.\"\n",
        "        msg_lower = message.lower().strip()\n",
        "\n",
        "        if msg_lower in (\"hi\", \"hello\", \"hey\"):\n",
        "            return \"üëã Hello! I'm your bill assistant. I can help you extract data from bills, answer questions, and export information.\"\n",
        "\n",
        "        if \"help\" in msg_lower:\n",
        "            help_text = \"üí° I can:\\n1. Process bill images and PDFs\\n2. Answer questions about bills\\n3. Export data to JSON\\n4. General chat\"\n",
        "            if self.current_bill:\n",
        "                help_text += \"\\n\\n‚úÖ Current bill loaded! Ask about totals, dates, items, or custom questions.\"\n",
        "            else:\n",
        "                help_text += \"\\n\\n‚ö†Ô∏è No bill processed yet. Please process a bill first to ask questions about it.\"\n",
        "            return help_text\n",
        "\n",
        "        # Handle queries when bill is loaded\n",
        "        if self.current_bill is not None:\n",
        "            return self.answer_query(message, top_k=3)\n",
        "\n",
        "        # No bill loaded - provide helpful guidance without LLM calls\n",
        "        if any(keyword in msg_lower for keyword in [\"bill\", \"invoice\", \"receipt\", \"document\", \"pdf\", \"image\", \"process\", \"upload\", \"load\", \"scan\"]):\n",
        "            return \"üìé Please process a bill first using option 1 in the main menu. I can handle images and PDFs!\"\n",
        "\n",
        "        if any(keyword in msg_lower for keyword in [\"thank\", \"bye\", \"exit\", \"goodbye\"]):\n",
        "            return \"üëã You can exit chat mode anytime by typing 'exit' or 'quit'.\"\n",
        "\n",
        "        # General fallback when no bill is loaded\n",
        "        return \"ü§ñ I'm ready to help with bills! Please process a bill first (option 1), then ask questions like 'What's the total?' or 'Show items'. Type 'help' for options.\"\n",
        "\n",
        "    def chat_loop(self):\n",
        "        \"Continuous chat loop until user exits.\"\n",
        "        print(\"\\nüí¨ Chat mode activated (type 'exit' or 'quit' to return to main menu)\")\n",
        "        print(\"ü§ñ Assistant: Hello! I'm your bill assistant. How can I help you today?\")\n",
        "\n",
        "        while True:\n",
        "            message = input(\"You: \").strip()\n",
        "\n",
        "            if message.lower() in ('exit', 'quit', 'bye', 'goodbye'):\n",
        "                print(\"üëã Exiting chat mode...\")\n",
        "                break\n",
        "\n",
        "            response = self.chat(message)\n",
        "            print(f\"ü§ñ Assistant: {response}\")\n",
        "\n",
        "    def run_cli(self):\n",
        "        \"\"\"Interactive command-line menu.\"\"\"\n",
        "        self.print_device_info()\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"BILL ASSISTANT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        print(\"\\n1. üì∏ Process bill image/PDF\")\n",
        "        print(\"2. üíæ Export to JSON\")\n",
        "        print(\"3. üí¨ Chat\")\n",
        "        print(\"4. üö™ Exit\")\n",
        "\n",
        "        while True:\n",
        "            choice = input(\"\\nSelect option (1-4): \").strip()\n",
        "\n",
        "            if choice == \"1\":\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                img_path = input(\"Enter image/PDF path (or URL): \").strip()\n",
        "                result = self.process_bill(img_path)\n",
        "                print(result)\n",
        "                if self.current_bill:\n",
        "                    print(\"\\nüìä Bill Summary:\")\n",
        "                    print(f\"Invoice: {self.current_bill.get('invoice_number', 'N/A')}\")\n",
        "                    print(f\"Date: {self.current_bill.get('invoice_date', 'N/A')}\")\n",
        "                    print(f\"Total: ${self.current_bill.get('total', 'N/A')}\")\n",
        "                print(\"\\n\" + \"=\" * 50)\n",
        "                print(\"BILL ASSISTANT\")\n",
        "                print(\"=\" * 50)\n",
        "                print(\"\\n1. üì∏ Process bill image/PDF\")\n",
        "                print(\"2. üíæ Export to JSON\")\n",
        "                print(\"3. üí¨ Chat\")\n",
        "                print(\"4. üö™ Exit\")\n",
        "\n",
        "            elif choice == \"2\":\n",
        "                filename = input(\"Enter filename (or press Enter for default): \").strip()\n",
        "                result = self.export_to_json(filename if filename else None)\n",
        "                print(result)\n",
        "\n",
        "            elif choice == \"3\":\n",
        "                self.chat_loop()  # Call the new continuous chat loop\n",
        "                # Show menu again after exiting chat\n",
        "                print(\"\\n\" + \"=\" * 50)\n",
        "                print(\"BILL ASSISTANT\")\n",
        "                print(\"=\" * 50)\n",
        "                print(\"\\n1. üì∏ Process bill image/PDF\")\n",
        "                print(\"2. üíæ Export to JSON\")\n",
        "                print(\"3. üí¨ Chat\")\n",
        "                print(\"4. üö™ Exit\")\n",
        "\n",
        "            elif choice == \"4\":\n",
        "                print(\"üëã Goodbye!\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"‚ùå Invalid choice!\")\n",
        "\n",
        "# # If run as script, start the CLI3\n",
        "# if __name__ == \"__main__\":\n",
        "#     assistant = BillAssistant(use_colab_secrets=True)\n",
        "#     assistant.run_cli()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSTvA6FHtf1e",
        "outputId": "b309ae61-98f1-4346-db9a-ca5f3fba807f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import tempfile\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from assistant import BillAssistant  # assume this is the provided class\n",
        "import hashlib\n",
        "\n",
        "# Cache the assistant so models load only once\n",
        "@st.cache_resource\n",
        "def load_assistant():\n",
        "    return BillAssistant(use_colab_secrets=True)\n",
        "\n",
        "assistant = load_assistant()\n",
        "\n",
        "st.set_page_config(page_title=\"Bill Assistant\", layout=\"centered\")\n",
        "st.title(\"Bill Assistant\")\n",
        "st.write(\"Upload a bill image (PNG/JPEG) or PDF below and then click **Process bill**. Processing will run once per uploaded file unless you clear it.\")\n",
        "\n",
        "# Initialize session state keys we use\n",
        "if \"uploaded_file_id\" not in st.session_state:\n",
        "    st.session_state.uploaded_file_id = None\n",
        "if \"temp_path\" not in st.session_state:\n",
        "    st.session_state.temp_path = None\n",
        "if \"process_result\" not in st.session_state:\n",
        "    st.session_state.process_result = None\n",
        "if \"processed_ok\" not in st.session_state:\n",
        "    st.session_state.processed_ok = False\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "def make_file_id(uploaded_file) -> str:\n",
        "    \"\"\"Create a simple id for uploaded file to avoid reprocessing same content.\"\"\"\n",
        "    # Use filename + size + sha1 of bytes for more certainty\n",
        "    try:\n",
        "        content = uploaded_file.getbuffer()\n",
        "        h = hashlib.sha1(content).hexdigest()\n",
        "        return f\"{uploaded_file.name}-{uploaded_file.size}-{h}\"\n",
        "    except Exception:\n",
        "        return f\"{uploaded_file.name}-{uploaded_file.size}\"\n",
        "\n",
        "def save_uploaded_to_temp(uploaded_file) -> str:\n",
        "    \"\"\"Save Streamlit uploaded file to a unique temp path and return the path.\"\"\"\n",
        "    suffix = \"\"\n",
        "    if uploaded_file.type == \"application/pdf\" or uploaded_file.name.lower().endswith(\".pdf\"):\n",
        "        suffix = \".pdf\"\n",
        "    else:\n",
        "        # try to preserve extension\n",
        "        ext = os.path.splitext(uploaded_file.name)[1] or \".png\"\n",
        "        suffix = ext if ext.startswith(\".\") else f\".{ext}\"\n",
        "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix, prefix=\"bill_\")\n",
        "    with open(tmp.name, \"wb\") as f:\n",
        "        f.write(uploaded_file.getbuffer())\n",
        "    return tmp.name\n",
        "\n",
        "# File uploader for image or PDF bills\n",
        "uploaded_file = st.file_uploader(\"Choose an image (PNG/JPEG) or PDF\", type=[\"png\", \"jpg\", \"jpeg\", \"pdf\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    file_id = make_file_id(uploaded_file)\n",
        "    if uploaded_file.type != \"application/pdf\":\n",
        "        try:\n",
        "            preview_image = Image.open(uploaded_file).convert(\"RGB\")\n",
        "            print(f\"Previewing image...\")\n",
        "            st.image(preview_image, caption=\"Preview\", width=\"content\")\n",
        "        except Exception:\n",
        "            st.write(\"Preview not available for this image.\")\n",
        "    else:\n",
        "        st.write(\"PDF uploaded. Preview not shown.\")\n",
        "\n",
        "    # If it's the same file already processed, show status and results without reprocessing\n",
        "    if st.session_state.uploaded_file_id == file_id and st.session_state.processed_ok:\n",
        "        st.success(\"This file was already processed in this session.\")\n",
        "    else:\n",
        "        # Show action buttons\n",
        "        cols = st.columns([1, 1, 1])\n",
        "        with cols[0]:\n",
        "            if st.button(\"Process bill\"):\n",
        "                # save to temp and process\n",
        "                tmp_path = save_uploaded_to_temp(uploaded_file)\n",
        "                st.session_state.temp_path = tmp_path\n",
        "                st.session_state.uploaded_file_id = file_id\n",
        "\n",
        "                # run processing with a spinner\n",
        "                with st.spinner(\"Processing bill (OCR + parsing)... this may take a while for large PDFs\"):\n",
        "                    # option: free GPU memory (harmless if not available)\n",
        "                    try:\n",
        "                        import torch, gc\n",
        "                        torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                    result = assistant.process_bill(tmp_path)\n",
        "                    st.session_state.process_result = result\n",
        "                    st.session_state.processed_ok = result.startswith(\"‚úÖ\")\n",
        "\n",
        "                if st.session_state.processed_ok:\n",
        "                    st.success(\"Bill processed successfully!\")\n",
        "                else:\n",
        "                    st.error(\"Processing failed: \" + (st.session_state.process_result or \"Unknown error\"))\n",
        "\n",
        "        with cols[1]:\n",
        "            if st.button(\"Clear / Reset\"):\n",
        "                # remove temp file if any\n",
        "                try:\n",
        "                    if st.session_state.temp_path and os.path.exists(st.session_state.temp_path):\n",
        "                        os.remove(st.session_state.temp_path)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                # reset session-state fields\n",
        "                st.session_state.uploaded_file_id = None\n",
        "                st.session_state.temp_path = None\n",
        "                st.session_state.process_result = None\n",
        "                st.session_state.processed_ok = False\n",
        "                st.session_state.messages = []\n",
        "                st.experimental_rerun()\n",
        "\n",
        "        with cols[2]:\n",
        "            if st.button(\"Show raw extracted text (if processed)\"):\n",
        "                if st.session_state.processed_ok and assistant.bill_text:\n",
        "                    st.text_area(\"Extracted text\", assistant.bill_text, height=300)\n",
        "                else:\n",
        "                    st.info(\"No extracted text available. Process the file first.\")\n",
        "\n",
        "# If processing was successful, show summary & download\n",
        "if st.session_state.processed_ok and assistant.current_bill:\n",
        "    bill = assistant.current_bill\n",
        "    st.write(\"---\")\n",
        "    st.header(\"Extracted bill summary\")\n",
        "    st.markdown(f\"**Invoice:** {bill.get('invoice_number', 'N/A')}\")\n",
        "    st.markdown(f\"**Date:** {bill.get('invoice_date', 'N/A')}\")\n",
        "    st.markdown(f\"**Total:** ${bill.get('total', 'N/A')}\")\n",
        "\n",
        "    items = bill.get(\"items\", [])\n",
        "    if items:\n",
        "        df_items = pd.DataFrame(items)\n",
        "        st.write(\"**Line Items:**\")\n",
        "        st.table(df_items)\n",
        "\n",
        "    bill_json = json.dumps(bill, indent=2)\n",
        "    st.download_button(\n",
        "        \"Download Bill as JSON\",\n",
        "        data=bill_json,\n",
        "        file_name=\"extracted_bill.json\",\n",
        "        mime=\"application/json\"\n",
        "    )\n",
        "\n",
        "# Chat interface for queries about the bill\n",
        "st.write(\"---\")\n",
        "st.write(\"### Ask questions about the bill or chit-chat\")\n",
        "\n",
        "# Display previous messages\n",
        "for msg in st.session_state.messages:\n",
        "    role = msg.get(\"role\", \"user\")\n",
        "    with st.chat_message(role):\n",
        "        st.markdown(msg[\"content\"])\n",
        "\n",
        "# Chat input\n",
        "user_input = st.chat_input(\"Your message...\")\n",
        "if user_input:\n",
        "    # Add user message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    # Get assistant response\n",
        "    response = assistant.chat(user_input)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "    # Display assistant's new message\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpSs_265f0rI",
        "outputId": "4d489a11-4e67-4824-c62d-2a624f1b378d"
      },
      "outputs": [],
      "source": [
        "!streamlit run main.py & npx localtunnel --port 8501 --y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2n5yOBF-K4t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
